# -*- coding: utf-8 -*-
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor
from tiebaData.items import PostItem,UserItem,TopicItem
import scrapy
import json

import re
import time

#分为主题topic和帖子post。无法显示图片内容
class tiebaSpider(CrawlSpider):
    #用于区别Spider。 该名字必须是唯一的，不可以为不同的Spider设定相同的名字。
    name = "tieba"
    #爬取的url允许的域名
    #allowed_domains = ["news.baidu"]
    #包含了Spider在启动时进行爬取的url列表。 因此，第一个被获取到的页面将是其中之一。 后续的URL则从初始的URL获取到的数据中提取。
    start_urls = ['http://tieba.baidu.com/f?kw=%E6%A2%A6%E4%B8%89%E5%9B%BD2&ie=utf-8']
    #start_urls = ['http://news.baidu.com/']
    #LinkExtractor是从网页(scrapy.http.Response 对象)中抽取最终将会被follow链接的对象，并且回调callback中指定的方法。
    rules = (
    
    Rule(LinkExtractor(allow=('http://tieba.baidu.com/f?kw=%E6%A2%A6%E4%B8%89%E5%9B%BD2&ie=utf-8&pn=\d+')),callback= 'parse_topic_page'),
    
    )
    
    baseurl = "http://tieba.baidu.com"
    
    #解析第一页的链接
    def parse(self,response):
        print '[URL****]',response.url
        link = response.xpath('//div[@class = "thread_list_bottom clearfix"]/div[@id = "frs_list_pager"]/a[@class="next pagination-item "]/@href').extract()[0]
        topic = scrapy.Request(link,callback=self.parse_topic_page)
        yield topic
        topics = scrapy.Request(response.url,callback=self.get_topic)
        yield topics
    
    #获取主题分页链接
    def parse_topic_page(self, response):
        #获取主题分页链接        
        print '[URL****]',response.url
        for sel in response.xpath('//div[@class="t_con cleafix"]/div[@class="col2_right j_threadlist_li_right "]/div[@class="threadlist_lz clearfix"]/div[@class="threadlist_title pull_left j_th_tit "]/a[@class = "j_th_tit "]/@href').extract():
            url = sel
            link = self.baseurl+url
            post = scrapy.Request(link,callback=self.parse_post_page)
            yield post
        #获取下一页帖子
        next_page_list = response.xpath('//div[@class = "thread_list_bottom clearfix"]/div[@id = "frs_list_pager"]/a[@class="next pagination-item "]/@href').extract()
        if len(next_page_list):
            next_page = next_page_list[0]
        else:
            print 'All Pages Are Downloaded.'
        request = scrapy.Request(next_page,callback=self.parse_topic_page)
        yield request
        #请求topic数据
        topics = scrapy.Request(response.url,callback=self.get_topic)
        yield topics
        
    #获取topic的数据
    def get_topic(self,response):
        print '[TOPIC CONTENT]',response.url
        for sel in response.xpath('//div/ul[@id="thread_list"]/li[@class=" j_thread_list clearfix"]'):
            topic_item = TopicItem()
            topic = sel.xpath('div[@class = "t_con cleafix"]')
            topic_id = re.search(r'\"id\"\:(\d+)\,.*',sel.xpath('@data-field').extract()[0]).group(1)
            topic_replies = topic.xpath('div[@class = "col2_left j_threadlist_li_left"]/span/text()').extract()
            topic_title = ''
            name = topic.xpath('div[@class = "col2_right j_threadlist_li_right "]/div[@class = "threadlist_lz clearfix"]/div[@class = "threadlist_title pull_left j_th_tit "]/a/text()').extract()
            if len(name):
                topic_title = name[0]
            topic_content = ''
            content = topic.xpath('div[@class = "col2_right j_threadlist_li_right "]/div[@class = "threadlist_detail clearfix"]/div[@class = "threadlist_text pull_left"]/div/text()').extract()
            if len(content):
                topic_content = content.strip()
            topic_author_name = ''
            author_name = topic.xpath('div[@class = "col2_right j_threadlist_li_right "]/div[@class = "threadlist_lz clearfix"]/div[@class = "threadlist_author pull_right"]/span/@title').extract()
            if len(author_name):
                topic_author_name = re.search(r'.*\:(.*)$',author_name[0]).group(1)
            author_id = topic.xpath('div[@class = "col2_right j_threadlist_li_right "]/div[@class = "threadlist_lz clearfix"]/div[@class = "threadlist_author pull_right"]/span[@class = "tb_icon_author "]/@data-field').extract()
            topic_author_id = ''
            if len(author_id):
                topic_author_id = re.search(r'.*\:(\d+)',author_id[0]).group(1)
            topic_time = topic.xpath('div[@class = "col2_right j_threadlist_li_right "]/div[@class = "threadlist_lz clearfix"]/div[@class = "threadlist_author pull_right"]/span[@class = "pull-right is_show_create_time"]/text()').extract()[0]
            
            topic_item['topic_id'] = topic_id
            topic_item['topic_title'] = topic_title
            topic_item['topic_content'] = topic_content
            topic_item['topic_author_name'] = topic_author_name
            topic_item['topic_author_id'] = topic_author_id
            topic_item['topic_replies'] = topic_replies
            topic_item['topic_time'] = topic_time
            topic_item['topic_link'] = response.url
            topic_item['record_time'] = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
            yield topic_item
            
    #获取帖子中分页链接
    def parse_post_page(self,response):
        #获取帖子的id
        topic_id = re.search(r'.*\/(\d+)',response.url).group(1)
        print '[TOPIC_URL]',response.url
        #获取帖子回复的页数
        pages = response.xpath('//div[@id="container"]/div[@class="content clearfix"]/div[@class="pb_footer"]/div/div/ul/li/a/@href').extract()
        max_page = ''
        if len(pages):
            max_page = re.search(r'.*pn=(\d+)',pages[-1]).group(1)
            for i in range(1,int(max_page)+1):
                post_page = self.baseurl+'/p/'+topic_id+'?pn='+str(i)
                print '[POST URL]',post_page
                #解析帖子回复的内容
                post_ans = scrapy.Request(post_page,callback=self.get_post)
                yield post_ans

    #获取帖子内容
    def get_post(self,response):
        
        post_link = response.url
        #https://tieba.baidu.com/p/3870871522?pn=2
        topic_id = re.search(r'.*\/(\d+)\?.*',post_link).group(1)
        
        for sel in response.xpath('//div[@id="container"]/div[@class="content clearfix"]/div[@class="pb_content clearfix"]/div[@class="left_section"]/div[@class="p_postlist"]/div[@class="l_post l_post_bright j_l_post clearfix  "]'):
            
            post_item = PostItem()
            
            #获取回复的用户主页链接
            user = sel.xpath('div[@class="d_author"]/ul[@class="p_author"]/li[@class="d_name"]')
            user_id = re.search(r'.*\:(\d+)',user.xpath('@data-field').extract()[0]).group(1)
            user_name = user.xpath('a/text()').extract()[0]
            user_link = self.baseurl+user.xpath('a/@href').extract()[0]

            #请求用户数据
            user = scrapy.Request(user_link,callback=self.get_user)
            user.meta['user_id'] = user_id
            yield user
            
            #获取回复的内容
            post = sel.xpath('div[@class="d_post_content_main "]/div[@class="p_content  "]/cc')
            post_contents = post.xpath('div/text()').extract()
            post_content = ''
            if len(post_contents):
                post_content = post_contents[0]
            
            #获取楼层的其它信息
            other = sel.xpath('div[@class="d_post_content_main "]/div[@class="core_reply j_lzl_wrapper"]/div[@class="core_reply_tail clearfix"]/div[@class="post-tail-wrap"]')
            post_device = other.xpath('span[1]/a/text()').extract()[0]
            post_floor = other.xpath('span[2]/text()').extract()[0]
            post_time = other.xpath('span[3]/text()').extract()[0]
            
            #获取楼层的回复信息
            for reply in sel.xpath('div[@class="d_post_content_main "]/div[@class="core_reply j_lzl_wrapper"]/div[@class="j_lzl_c_b_a core_reply_content"]/div[@class="j_lzl_c_b_a core_reply_content"]/ul/li'):
                reply_content = reply.xpath('div[@class="lzl_cnt"]/span/text()').extract()[0]
                reply_user = reply.xpath('div[@class="lzl_cnt"]/a/text()').extract()[0]
            
            #每个回复都针对一个帖子
            post_item['post_to'] = topic_id
            post_item['topic_id'] = topic_id
            post_item['post_user_id'] = user_id
            post_item['post_user_name'] = user_name.strip()
            post_item['post_content'] = post_content.strip()
            post_item['post_link'] = post_link
            post_item['record_time'] = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
            yield post_item
            
            
            
            
        #通过获取data-field
        for post in response.xpath('//*[@id="j_p_postlist"]/div'):
            data-field = post.xpath('@data-field').extract()
            data = json.loads(data-field)
            post_user_id = data['author']['user_id']
            post_user_name = data['author']['user_name']
            post_id = data['content']['post_id']
            post_to = data['content']['thread_id']
            post_content = re.search(r'(.*)\<img.*$',data['content']['content']).group(1)
            
    
    #获取用户信息
    def get_user(self,response):
    
        user_item = UserItem()
        user_id = response.meta['user_id']
        user_item['user_id'] = user_id
        user_item['user_link'] = response.url
        vip_info = response.xpath('//div[@id="userinfo_wrap"]/div[@class="userinfo_middle"]/div[@class="userinfo_title"]/span').extract()
        isvip = ''
        if len(vip_info) == 1:
            isvip = '1'
        else:
            isvip = '0'
            
        user_item['user_vip'] = isvip
        user_item['user_name'] = response.xpath('//div[@id="userinfo_wrap"]/div[@class="userinfo_middle"]/div[@class="userinfo_title"]/div[@class="userinfo-marry"]/@data-username').extract()[0]
        user = response.xpath('//div[@id="userinfo_wrap"]/div[@class="userinfo_middle"]/div[@class="userinfo_num"]/div[@class="userinfo_userdata"]')
        user_sex = user.xpath('span[1]/@class').extract()[0]
        user_post_age = user.xpath('span[2]/text()').extract()[0]
        user_post_num = user.xpath('span[4]/text()').extract()[0]
        
        other = response.xpath('//div[@id="container"]/div[@class="right_aside"]/div[@class="ihome_aside_section"]/h1[@class="ihome_aside_title"]/span[@class="concern_num"]')
        
        fans = response.xpath('//div[@id="container"]/div[@class="right_aside"]/div[3]/h1[@class="ihome_aside_title"]/span[@class="concern_num"]/a/text()').extract()
        if len(fans):
            user_item['user_fans'] = fans[0]
        else:
            user_item['user_fans'] = '0'
        
        forks = other.xpath('a[1]/text()').extract()
        if len(forks):
            user_item['user_fork'] = forks[0]
        else:
            user_item['user_fork'] = '0'
        user_item['user_sex'] = re.search(r'.*userinfo_sex\_(.*)$',user_sex).group(1)
        user_item['user_post_age'] = user_post_age
        user_item['user_post_num'] = user_post_num
        user_item['record_time'] = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
        yield user_item
        
        
    